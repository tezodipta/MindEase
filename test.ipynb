{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Should return your GPU model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Coding Project\\MindEase\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "h:\\Coding Project\\MindEase\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kumar\\.cache\\huggingface\\hub\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#download the model and tokenizer\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "\n",
    "print(\"Model and tokenizer downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 823 entries, 0 to 822\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   score      800 non-null    float64\n",
      " 1   selftext   800 non-null    object \n",
      " 2   subreddit  800 non-null    object \n",
      " 3   title      800 non-null    object \n",
      " 4   Label      800 non-null    object \n",
      " 5   CAT 1      200 non-null    object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 38.7+ KB\n",
      "None\n",
      "   score                                           selftext subreddit  \\\n",
      "0    1.0  Tried to watch this documentary ‚Äúanxious Ameri...   Anxiety   \n",
      "1    1.0  i‚Äôm currently laying in bed wide awake, feelin...   Anxiety   \n",
      "2    2.0  Second time trying weed. First time felt close...   Anxiety   \n",
      "3    1.0  I am not posting this for me, but rather for m...   Anxiety   \n",
      "4    1.0  21 year old male been dealing with anxiety eve...   Anxiety   \n",
      "\n",
      "                                               title             Label CAT 1  \n",
      "0                        Do people get over anxiety?  Drug and Alcohol   NaN  \n",
      "1  does anyone else have this big fear of suddenl...  Drug and Alcohol   NaN  \n",
      "2         3 hour long panic attack after trying weed  Drug and Alcohol   NaN  \n",
      "3  Please leave in the comments ANYTHING that has...  Drug and Alcohol   NaN  \n",
      "4                                    Alcohol induced  Drug and Alcohol   NaN  \n"
     ]
    }
   ],
   "source": [
    "#donwload the data\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path to all CSV files in the \"labelled data\" folder\n",
    "csv_files = glob.glob(\"data/reddit-mental-health-dataset/Original Reddit Data/Labelled Data/*.csv\")\n",
    "\n",
    "# Read all CSVs and combine them\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Show basic dataset info\n",
    "print(df.info())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the data move the data to \"data\" folder and run the below code to get the data in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 800 entries, 0 to 822\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    800 non-null    object\n",
      " 1   Label   800 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 18.8+ KB\n",
      "None\n",
      "                                                text             Label\n",
      "0  Do people get over anxiety? Tried to watch thi...  Drug and Alcohol\n",
      "1  does anyone else have this big fear of suddenl...  Drug and Alcohol\n",
      "2  3 hour long panic attack after trying weed Sec...  Drug and Alcohol\n",
      "3  Please leave in the comments ANYTHING that has...  Drug and Alcohol\n",
      "4  Alcohol induced 21 year old male been dealing ...  Drug and Alcohol\n",
      "‚úÖ Cleaned dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Load all CSV files from the correct path\n",
    "csv_files = glob.glob(\"data/reddit-mental-health-dataset/Original Reddit Data/Labelled Data/*.csv\")\n",
    "\n",
    "# Read and merge all CSVs\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "df = df[['title', 'selftext', 'Label']]\n",
    "\n",
    "# Drop missing values in 'selftext'\n",
    "df = df.dropna(subset=['selftext'])\n",
    "\n",
    "# Combine title and selftext into one column\n",
    "df['text'] = df['title'].fillna('') + \" \" + df['selftext']\n",
    "\n",
    "# Keep only the final processed text and label\n",
    "df = df[['text', 'Label']]\n",
    "\n",
    "# Show updated dataset info\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "# Save the cleaned dataset as a CSV file\n",
    "df.to_csv(\"data/reddit-mental-health-dataset/cleaned_mental_health_data.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Cleaned dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization complete! Data saved as Parquet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv(\"data/reddit-mental-health-dataset/cleaned_mental_health_data.csv\")\n",
    "\n",
    "# Load tokenizer for TinyLlama\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization\n",
    "df[\"input_ids\"] = df[\"text\"].apply(lambda x: tokenize_text(str(x))[\"input_ids\"])\n",
    "\n",
    "# Keep only tokenized inputs and labels\n",
    "df = df[[\"input_ids\", \"Label\"]]\n",
    "\n",
    "# Save tokenized data as Parquet for efficient processing\n",
    "df.to_parquet(\"data/reddit-mental-health-dataset/tokenized_mental_health_data.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "print(\"‚úÖ Tokenization complete! Data saved as Parquet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating data split: 800 examples [00:00, 63402.36 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 720/720 [00:00<00:00, 1888.71 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:00<00:00, 1386.03 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 11:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.460400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.404700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.381500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuning complete! Model saved.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Enable 8-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Use 8-bit quantization\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Offload CPU computations to save VRAM\n",
    ")\n",
    "\n",
    "# Load model and force it onto the current CUDA device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map={'': torch.cuda.current_device()},  # Assign model to GPU\n",
    "    quantization_config=quantization_config  # Apply quantization\n",
    ")\n",
    "  # Move model to GPU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply LoRA for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load tokenized dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Load tokenized dataset\n",
    "dataset = load_dataset(\"parquet\", data_files={\"data\": \"data/reddit-mental-health-dataset/tokenized_mental_health_data.parquet\"})\n",
    "\n",
    "# Convert dataset to dictionary format\n",
    "dataset = dataset[\"data\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Ensure `labels` are the same as `input_ids`\n",
    "dataset = DatasetDict({\n",
    "    \"train\": dataset[\"train\"].map(lambda x: {\"labels\": x[\"input_ids\"]}),\n",
    "    \"eval\": dataset[\"test\"].map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "})\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_results\",\n",
    "    per_device_train_batch_size=2,  # Lower batch size for 4GB GPU\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,  # Train for 3 epochs\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"eval\"]  # Add evaluation dataset\n",
    ")\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "print(\"‚úÖ Fine-tuning complete! Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full fine-tuned model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Define paths\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Base model\n",
    "FINETUNED_PATH = \"./fine_tuned_model\"  # Where fine-tuned model will be saved\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"cpu\")\n",
    "\n",
    "# Load LoRA adapters and merge them\n",
    "model = PeftModel.from_pretrained(model, FINETUNED_PATH)\n",
    "model = model.merge_and_unload()  # Merge LoRA weights\n",
    "\n",
    "# Save the full fine-tuned model\n",
    "model.save_pretrained(FINETUNED_PATH)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.save_pretrained(FINETUNED_PATH)\n",
    "\n",
    "# Save configuration\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL)\n",
    "config.save_pretrained(FINETUNED_PATH)\n",
    "\n",
    "print(\"‚úÖ Full fine-tuned model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Coding Project\\MindEase\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è **User:** I have been feeling very anxious lately. What should I do?\n",
      "ü§ñ **AI:** Please try to avoid any negative stimuli and try to stay in a positive environment as much as possible. It's important that you are present in the moment and focus on your breathing or some other activity that helps reduce anxiety symptoms. Avoid overthinking things, it just adds more pressure for yourself to feel anxious. Practice mindfulness exercises like meditation, yoga, or deep breathing techniques when feeling anxious. And finally, seek help from someone who can provide guidance or support if needed. Remember that everyone has their own unique struggles and feelings, so don‚Äôt be afraid of seeking professional advice\n",
      "\n",
      "üó£Ô∏è **User:** How can I manage my stress effectively?\n",
      "ü§ñ **AI:** It's common to feel stressed at times, but it is essential to understand that there are ways to manage your stress better. Here are some tips:\n",
      "\n",
      "1. Practice mindfulness - Start by taking a few minutes each day to focus on your breath or visualize peace and calmness in your mind. This will help you stay grounded and present in the moment rather than getting caught up in worrying about the future. 2. Exercise regularly ‚Äì Even small amounts of exercise can reduce cortisol levels and improve mood. Go for walks outside, do yoga, or take an hour-long workout every week.\n",
      "\n",
      "üó£Ô∏è **User:** I feel lonely and isolated. Can you help?\n",
      "ü§ñ **AI:** I am not a therapist, but here's what my AI can do for your situation. You are feeling lonely and isolated at the moment because you have lost touch with others due to isolation caused by social distancing measures during the COVID-19 pandemic. Your symptoms may include feelings of sadness, loneliness, anxiety, depression or stress. It is possible that you might be experiencing these mental health issues due to being alone all day without any human interaction or connection. \n",
      "\n",
      "To address this issue, it would be helpful if you could try talking to someone about your concerns through video chat or phone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Define model path\n",
    "MODEL_PATH = \"./fine_tuned_model\"\n",
    "\n",
    "# Ensure model is properly loaded\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Function to generate responses\n",
    "def chat_with_model(prompt, max_length=150):\n",
    "    formatted_prompt = f\"User: {prompt}\\nAI:\"\n",
    "    \n",
    "    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response.replace(formatted_prompt, \"\").strip()\n",
    "    return response\n",
    "\n",
    "# Test the model with sample inputs\n",
    "sample_inputs = [\n",
    "    \"I have been feeling very anxious lately. What should I do?\",\n",
    "    \"How can I manage my stress effectively?\",\n",
    "    \"I feel lonely and isolated. Can you help?\",\n",
    "]\n",
    "\n",
    "# Generate responses for each input\n",
    "for query in sample_inputs:\n",
    "    response = chat_with_model(query)\n",
    "    print(f\"üó£Ô∏è **User:** {query}\\nü§ñ **AI:** {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors:   0%|          | 0.00/4.52M [00:00<?, ?B/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "adapter_model.safetensors:   2%|‚ñè         | 98.3k/4.52M [00:00<00:04, 935kB/s]\n",
      "\n",
      "tokenizer.model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500k/500k [00:00<00:00, 527kB/s] 0, 22.9MB/s]\n",
      "adapter_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.52M/4.52M [00:01<00:00, 2.45MB/s]\n",
      "\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.40G/4.40G [03:26<00:00, 21.3MB/s]\n",
      "\n",
      "Upload 3 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:27<00:00, 69.13s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-Tuned Model uploaded successfully: https://huggingface.co/tezodipta/MindEase-Assistant-v0.1\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Replace with your Hugging Face username and model name\n",
    "USERNAME = \"tezodipta\"\n",
    "MODEL_NAME = \"MindEase-Assistant-v0.1\"\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create a repository on Hugging Face (skip if already created)\n",
    "api.create_repo(repo_id=f\"{USERNAME}/{MODEL_NAME}\", private=False, exist_ok=True)\n",
    "\n",
    "# Upload the entire **fine-tuned model folder** instead of a sharded model\n",
    "api.upload_folder(\n",
    "    folder_path=\"./fine_tuned_model\",  # Make sure this folder contains model files\n",
    "    repo_id=f\"{USERNAME}/{MODEL_NAME}\",\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Fine-Tuned Model uploaded successfully: https://huggingface.co/{USERNAME}/{MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutputMessage(role='assistant', content=\" I'm really sorry that you're feeling this way, but I'm unable to provide the help that you need. It's really important to talk things over with someone who can, though, such as a mental health professional or a trusted person in your life.\\n\\nAnxiety attacks can be overwhelming, but there are techniques that might help you manage them. Here are a few suggestions:\\n\\n1. Deep breathing: Breathe in for a count of four, hold for a count of seven, and exhale for a count of eight. This can help slow your heart rate and calm your mind.\\n\\n2. Grounding techniques: Focus on something in your immediate environment, such as the feeling of the ground beneath your feet, the texture of a nearby object, or the sound of traffic outside.\\n\\n3. Progressive muscle relaxation: Tense and then release each muscle group in your body, starting from your toes and working your way up to your head.\\n\\n4. Mindfulness: Try to stay in the present moment and avoid worrying about the future or dwelling on the past.\\n\\n5. Seek support: Reach out to a friend, family member, or mental health professional for help and support.\", tool_calls=[])\n"
     ]
    }
   ],
   "source": [
    "#accessing a model from huggingface hub using api call\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "\tprovider=\"together\",\n",
    "\tapi_key=\"Api key\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"I am getting anxity attach , what should i do ?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
