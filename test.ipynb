{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Should return your GPU model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Coding Project\\MindEase\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "h:\\Coding Project\\MindEase\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kumar\\.cache\\huggingface\\hub\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#download the model and tokenizer\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "\n",
    "print(\"Model and tokenizer downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 823 entries, 0 to 822\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   score      800 non-null    float64\n",
      " 1   selftext   800 non-null    object \n",
      " 2   subreddit  800 non-null    object \n",
      " 3   title      800 non-null    object \n",
      " 4   Label      800 non-null    object \n",
      " 5   CAT 1      200 non-null    object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 38.7+ KB\n",
      "None\n",
      "   score                                           selftext subreddit  \\\n",
      "0    1.0  Tried to watch this documentary “anxious Ameri...   Anxiety   \n",
      "1    1.0  i’m currently laying in bed wide awake, feelin...   Anxiety   \n",
      "2    2.0  Second time trying weed. First time felt close...   Anxiety   \n",
      "3    1.0  I am not posting this for me, but rather for m...   Anxiety   \n",
      "4    1.0  21 year old male been dealing with anxiety eve...   Anxiety   \n",
      "\n",
      "                                               title             Label CAT 1  \n",
      "0                        Do people get over anxiety?  Drug and Alcohol   NaN  \n",
      "1  does anyone else have this big fear of suddenl...  Drug and Alcohol   NaN  \n",
      "2         3 hour long panic attack after trying weed  Drug and Alcohol   NaN  \n",
      "3  Please leave in the comments ANYTHING that has...  Drug and Alcohol   NaN  \n",
      "4                                    Alcohol induced  Drug and Alcohol   NaN  \n"
     ]
    }
   ],
   "source": [
    "#donwload the data\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path to all CSV files in the \"labelled data\" folder\n",
    "csv_files = glob.glob(\"data/reddit-mental-health-dataset/Original Reddit Data/Labelled Data/*.csv\")\n",
    "\n",
    "# Read all CSVs and combine them\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Show basic dataset info\n",
    "print(df.info())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the data move the data to \"data\" folder and run the below code to get the data in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 800 entries, 0 to 822\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    800 non-null    object\n",
      " 1   Label   800 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 18.8+ KB\n",
      "None\n",
      "                                                text             Label\n",
      "0  Do people get over anxiety? Tried to watch thi...  Drug and Alcohol\n",
      "1  does anyone else have this big fear of suddenl...  Drug and Alcohol\n",
      "2  3 hour long panic attack after trying weed Sec...  Drug and Alcohol\n",
      "3  Please leave in the comments ANYTHING that has...  Drug and Alcohol\n",
      "4  Alcohol induced 21 year old male been dealing ...  Drug and Alcohol\n",
      "✅ Cleaned dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Load all CSV files from the correct path\n",
    "csv_files = glob.glob(\"data/reddit-mental-health-dataset/Original Reddit Data/Labelled Data/*.csv\")\n",
    "\n",
    "# Read and merge all CSVs\n",
    "df_list = [pd.read_csv(file) for file in csv_files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Keep only relevant columns\n",
    "df = df[['title', 'selftext', 'Label']]\n",
    "\n",
    "# Drop missing values in 'selftext'\n",
    "df = df.dropna(subset=['selftext'])\n",
    "\n",
    "# Combine title and selftext into one column\n",
    "df['text'] = df['title'].fillna('') + \" \" + df['selftext']\n",
    "\n",
    "# Keep only the final processed text and label\n",
    "df = df[['text', 'Label']]\n",
    "\n",
    "# Show updated dataset info\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "# Save the cleaned dataset as a CSV file\n",
    "df.to_csv(\"data/reddit-mental-health-dataset/cleaned_mental_health_data.csv\", index=False)\n",
    "\n",
    "print(\"✅ Cleaned dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenization complete! Data saved as Parquet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv(\"data/reddit-mental-health-dataset/cleaned_mental_health_data.csv\")\n",
    "\n",
    "# Load tokenizer for TinyLlama\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_text(text):\n",
    "    return tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization\n",
    "df[\"input_ids\"] = df[\"text\"].apply(lambda x: tokenize_text(str(x))[\"input_ids\"])\n",
    "\n",
    "# Keep only tokenized inputs and labels\n",
    "df = df[[\"input_ids\", \"Label\"]]\n",
    "\n",
    "# Save tokenized data as Parquet for efficient processing\n",
    "df.to_parquet(\"data/reddit-mental-health-dataset/tokenized_mental_health_data.parquet\", engine=\"pyarrow\")\n",
    "\n",
    "print(\"✅ Tokenization complete! Data saved as Parquet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating data split: 800 examples [00:00, 63402.36 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 1888.71 examples/s]\n",
      "Map: 100%|██████████| 80/80 [00:00<00:00, 1386.03 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 11:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.460400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.404700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.381500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuning complete! Model saved.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Enable 8-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Use 8-bit quantization\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # Offload CPU computations to save VRAM\n",
    ")\n",
    "\n",
    "# Load model and force it onto the current CUDA device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map={'': torch.cuda.current_device()},  # Assign model to GPU\n",
    "    quantization_config=quantization_config  # Apply quantization\n",
    ")\n",
    "  # Move model to GPU\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply LoRA for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load tokenized dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Load tokenized dataset\n",
    "dataset = load_dataset(\"parquet\", data_files={\"data\": \"data/reddit-mental-health-dataset/tokenized_mental_health_data.parquet\"})\n",
    "\n",
    "# Convert dataset to dictionary format\n",
    "dataset = dataset[\"data\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Ensure `labels` are the same as `input_ids`\n",
    "dataset = DatasetDict({\n",
    "    \"train\": dataset[\"train\"].map(lambda x: {\"labels\": x[\"input_ids\"]}),\n",
    "    \"eval\": dataset[\"test\"].map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "})\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_results\",\n",
    "    per_device_train_batch_size=2,  # Lower batch size for 4GB GPU\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,  # Train for 3 epochs\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"eval\"]  # Add evaluation dataset\n",
    ")\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "print(\"✅ Fine-tuning complete! Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Full fine-tuned model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Define paths\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Base model\n",
    "FINETUNED_PATH = \"./fine_tuned_model\"  # Where fine-tuned model will be saved\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"cpu\")\n",
    "\n",
    "# Load LoRA adapters and merge them\n",
    "model = PeftModel.from_pretrained(model, FINETUNED_PATH)\n",
    "model = model.merge_and_unload()  # Merge LoRA weights\n",
    "\n",
    "# Save the full fine-tuned model\n",
    "model.save_pretrained(FINETUNED_PATH)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.save_pretrained(FINETUNED_PATH)\n",
    "\n",
    "# Save configuration\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL)\n",
    "config.save_pretrained(FINETUNED_PATH)\n",
    "\n",
    "print(\"✅ Full fine-tuned model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Coding Project\\MindEase\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗣️ **User:** I have been feeling very anxious lately. What should I do?\n",
      "🤖 **AI:** Please try to avoid any negative stimuli and try to stay in a positive environment as much as possible. It's important that you are present in the moment and focus on your breathing or some other activity that helps reduce anxiety symptoms. Avoid overthinking things, it just adds more pressure for yourself to feel anxious. Practice mindfulness exercises like meditation, yoga, or deep breathing techniques when feeling anxious. And finally, seek help from someone who can provide guidance or support if needed. Remember that everyone has their own unique struggles and feelings, so don’t be afraid of seeking professional advice\n",
      "\n",
      "🗣️ **User:** How can I manage my stress effectively?\n",
      "🤖 **AI:** It's common to feel stressed at times, but it is essential to understand that there are ways to manage your stress better. Here are some tips:\n",
      "\n",
      "1. Practice mindfulness - Start by taking a few minutes each day to focus on your breath or visualize peace and calmness in your mind. This will help you stay grounded and present in the moment rather than getting caught up in worrying about the future. 2. Exercise regularly – Even small amounts of exercise can reduce cortisol levels and improve mood. Go for walks outside, do yoga, or take an hour-long workout every week.\n",
      "\n",
      "🗣️ **User:** I feel lonely and isolated. Can you help?\n",
      "🤖 **AI:** I am not a therapist, but here's what my AI can do for your situation. You are feeling lonely and isolated at the moment because you have lost touch with others due to isolation caused by social distancing measures during the COVID-19 pandemic. Your symptoms may include feelings of sadness, loneliness, anxiety, depression or stress. It is possible that you might be experiencing these mental health issues due to being alone all day without any human interaction or connection. \n",
      "\n",
      "To address this issue, it would be helpful if you could try talking to someone about your concerns through video chat or phone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Define model path\n",
    "MODEL_PATH = \"./fine_tuned_model\"\n",
    "\n",
    "# Ensure model is properly loaded\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Function to generate responses\n",
    "def chat_with_model(prompt, max_length=150):\n",
    "    formatted_prompt = f\"User: {prompt}\\nAI:\"\n",
    "    \n",
    "    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response.replace(formatted_prompt, \"\").strip()\n",
    "    return response\n",
    "\n",
    "# Test the model with sample inputs\n",
    "sample_inputs = [\n",
    "    \"I have been feeling very anxious lately. What should I do?\",\n",
    "    \"How can I manage my stress effectively?\",\n",
    "    \"I feel lonely and isolated. Can you help?\",\n",
    "]\n",
    "\n",
    "# Generate responses for each input\n",
    "for query in sample_inputs:\n",
    "    response = chat_with_model(query)\n",
    "    print(f\"🗣️ **User:** {query}\\n🤖 **AI:** {response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors:   0%|          | 0.00/4.52M [00:00<?, ?B/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "adapter_model.safetensors:   2%|▏         | 98.3k/4.52M [00:00<00:04, 935kB/s]\n",
      "\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 527kB/s] 0, 22.9MB/s]\n",
      "adapter_model.safetensors: 100%|██████████| 4.52M/4.52M [00:01<00:00, 2.45MB/s]\n",
      "\n",
      "model.safetensors: 100%|██████████| 4.40G/4.40G [03:26<00:00, 21.3MB/s]\n",
      "\n",
      "Upload 3 LFS files: 100%|██████████| 3/3 [03:27<00:00, 69.13s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-Tuned Model uploaded successfully: https://huggingface.co/tezodipta/MindEase-Assistant-v0.1\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Replace with your Hugging Face username and model name\n",
    "USERNAME = \"tezodipta\"\n",
    "MODEL_NAME = \"MindEase-Assistant-v0.1\"\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Create a repository on Hugging Face (skip if already created)\n",
    "api.create_repo(repo_id=f\"{USERNAME}/{MODEL_NAME}\", private=False, exist_ok=True)\n",
    "\n",
    "# Upload the entire **fine-tuned model folder** instead of a sharded model\n",
    "api.upload_folder(\n",
    "    folder_path=\"./fine_tuned_model\",  # Make sure this folder contains model files\n",
    "    repo_id=f\"{USERNAME}/{MODEL_NAME}\",\n",
    ")\n",
    "\n",
    "print(f\"✅ Fine-Tuned Model uploaded successfully: https://huggingface.co/{USERNAME}/{MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://huggingface.co/api/inference-proxy/together/v1/chat/completions (Request ID: Root=1-67b42900-6d2a06a35b2b42a70a92e0fd;9ee54482-c3b7-40f2-8a7c-c308fa50d0c1)\n\nYou have exceeded your monthly included credits for Inference Endpoints. Subscribe to PRO to get 20x more monthly allowance.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\FinalYearProject\\All codes and sketchs\\MideEase\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\FinalYearProject\\All codes and sketchs\\MideEase\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://huggingface.co/api/inference-proxy/together/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m      8\u001b[0m client \u001b[38;5;241m=\u001b[39m InferenceClient(\n\u001b[0;32m      9\u001b[0m \tprovider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogether\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m \tapi_key\u001b[38;5;241m=\u001b[39mapi_key\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m \t{\n\u001b[0;32m     15\u001b[0m \t\t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m \t\t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \t}\n\u001b[0;32m     18\u001b[0m ]\n\u001b[1;32m---> 20\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistralai/Mistral-7B-Instruct-v0.3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32md:\\FinalYearProject\\All codes and sketchs\\MideEase\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:970\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[1;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[0;32m    943\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload_model,\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    962\u001b[0m }\n\u001b[0;32m    963\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[0;32m    964\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    965\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    968\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[0;32m    969\u001b[0m )\n\u001b[1;32m--> 970\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32md:\\FinalYearProject\\All codes and sketchs\\MideEase\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:327\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32md:\\FinalYearProject\\All codes and sketchs\\MideEase\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://huggingface.co/api/inference-proxy/together/v1/chat/completions (Request ID: Root=1-67b42900-6d2a06a35b2b42a70a92e0fd;9ee54482-c3b7-40f2-8a7c-c308fa50d0c1)\n\nYou have exceeded your monthly included credits for Inference Endpoints. Subscribe to PRO to get 20x more monthly allowance."
     ]
    }
   ],
   "source": [
    "#accessing a model from huggingface hub using api call\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Fetch the API key from a file named api_key.txt\n",
    "with open(\"api.txt\", \"r\") as file:\n",
    "\tapi_key = file.read().strip()\n",
    "\n",
    "client = InferenceClient(\n",
    "\tprovider=\"together\",\n",
    "\tapi_key=api_key\n",
    ")\n",
    "\n",
    "messages = [\n",
    "\t{\n",
    "\t\t\"role\": \"user\",\n",
    "\t\t\"content\": \"What is the capital of France?\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting together\n",
      "  Downloading together-1.4.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from together) (3.11.12)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from together) (8.1.8)\n",
      "Collecting eval-type-backport<0.3.0,>=0.1.3 (from together)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from together) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from together) (2.1.2)\n",
      "Collecting pillow<12.0.0,>=11.1.0 (from together)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pyarrow>=10.0.1 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from together) (19.0.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from together) (2.10.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from together) (2.32.3)\n",
      "Collecting rich<14.0.0,>=13.8.1 (from together)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tabulate<0.10.0,>=0.9.0 (from together)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from together) (4.67.1)\n",
      "Collecting typer<0.16,>=0.9 (from together)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.18.3)\n",
      "Requirement already satisfied: colorama in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from click<9.0.0,>=8.1.7->together) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.6.3->together) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.6.3->together) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.31.0->together) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.31.0->together) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.31.0->together) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.31.0->together) (2025.1.31)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=13.8.1->together)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from rich<14.0.0,>=13.8.1->together) (2.19.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<0.16,>=0.9->together)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading together-1.4.1-py3-none-any.whl (80 kB)\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.6 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.8/2.6 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 1.0/2.6 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 1.6/2.6 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.8/2.6 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.6 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.4/2.6 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 1.3 MB/s eta 0:00:00\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: tabulate, shellingham, pillow, mdurl, eval-type-backport, markdown-it-py, rich, typer, together\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.0.0\n",
      "    Uninstalling pillow-11.0.0:\n",
      "      Successfully uninstalled pillow-11.0.0\n",
      "Successfully installed eval-type-backport-0.2.2 markdown-it-py-3.0.0 mdurl-0.1.2 pillow-11.1.0 rich-13.9.4 shellingham-1.5.4 tabulate-0.9.0 together-1.4.1 typer-0.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The city that never sleeps! New York has endless options for entertainment, culture, and adventure. Here are some fun things to do in New York:\n",
      "\n",
      "**Iconic Landmarks:**\n",
      "\n",
      "1. Statue of Liberty and Ellis Island: Take a ferry to Liberty Island to see the iconic statue up close and visit the Ellis Island Immigration Museum.\n",
      "2. Central Park: Explore the park's many walking paths, lakes, and landmarks like the Bethesda Fountain and Loeb Boathouse.\n",
      "3. Times Square: Experience the bright lights and energy of the \"Crossroads of the World.\"\n",
      "4. Empire State Building: Enjoy panoramic views of the city from the observation deck on the 86th floor.\n",
      "5. Brooklyn Bridge: Walk or bike across the iconic bridge for spectacular city views.\n",
      "\n",
      "**Museums and Galleries:**\n",
      "\n",
      "1. The Metropolitan Museum of Art: One of the world's largest and most renowned museums, with a collection that spans over 5,000 years of human history.\n",
      "2.\n"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "\n",
    "with open(\"together_key.txt\", \"r\") as file:\n",
    "\tapi_key = file.read().strip()\n",
    "# Initialize the client with API key\n",
    "client = Together(api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What are some fun things to do in New York?\"}],\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from groq) (2.10.6)\n",
      "Requirement already satisfied: sniffio in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\finalyearproject\\all codes and sketchs\\mideease\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
      "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. It's nice to meet you. Is there something I can help you with or would you like to chat?"
     ]
    }
   ],
   "source": [
    "#gorq api\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "with open(\"gorq_key.txt\", \"r\") as file:\n",
    "\tapi_key = file.read().strip()\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"hello\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question \"What is the meaning of life?\" is one of the most profound and enduring philosophical inquiries. Different cultures, religions, and philosophical traditions offer a variety of answers. Here are a few perspectives:\n",
      "\n",
      "1. **Existentialism**: Existentialists like Jean-Paul Sartre argued that life has no inherent meaning, and it is up to each individual to create their own purpose.\n",
      "\n",
      "2. **Religious and Spiritual Views**: Many religions provide their own interpretations. For example:\n",
      "   - In Christianity\n"
     ]
    }
   ],
   "source": [
    "#open router api\n",
    "from openai import OpenAI\n",
    "with open(\"openrouter_key.txt\", \"r\") as file:\n",
    "\tapi_key = file.read().strip()\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=api_key,\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  extra_body={},\n",
    "  model=\"mistralai/mistral-saba\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is the meaning of life?\"\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=100,\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
